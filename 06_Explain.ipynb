{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \"Skater\" related functions\n",
    "%matplotlib inline\n",
    "from skater.util.image_ops import load_image, show_image, normalize, add_noise, flip_pixels, image_transformation\n",
    "from skater.util.image_ops import in_between, greater_than, greater_than_or_equal, equal_to\n",
    "from skater.core.local_interpretation.dnni.deep_interpreter import DeepInterpreter\n",
    "from skater.core.visualizer.image_relevance_visualizer import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_level = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "mnist = input_data.read_data_sets(\"/tmp/\", one_hot=True)\n",
    "tf.logging.set_verbosity(current_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.005\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input as tensors\n",
    "X = tf.placeholder(\"float\", [None, num_input] , name=\"input\")\n",
    "Y = tf.placeholder(\"float\", [None, num_classes], name=\"output\")\n",
    "\n",
    "# weights and biases for each Layer\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1], mean=0.0, stddev=0.05)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], mean=0.0, stddev=0.05)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes], mean=0.0, stddev=0.05))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.zeros([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, act=tf.nn.relu): \n",
    "    layer_1 = act(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    layer_2 = act(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'], name=\"absolute_output\")\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1, num_steps+1):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % 100 == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "        print(\"Step {} Minibatch Loss= {:.4f} Training Accuracy= {:.3f}\".format(step, loss, acc))\n",
    "\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for MNIST test images\n",
    "test_x = mnist.test.images\n",
    "test_y = mnist.test.labels\n",
    "\n",
    "print(\"Test accuracy:\", sess.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "explain_model = './explanations/models/simple_mnist-model'\n",
    "\n",
    "import os\n",
    "os.makedirs(explain_model, exist_ok=True)\n",
    "\n",
    "saver.save(sess, explain_model, global_step=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 189\n",
    "input_x_i = test_x[[test_idx]]\n",
    "input_y_i = test_y[test_idx].reshape(1, 10)\n",
    "with DeepInterpreter(session=sess) as di:\n",
    "    # 1. Restore the persisted model\n",
    "    # 2. Retrieve the input tensor from the restored model\n",
    "    saver = tf.train.import_meta_graph('./explanations/models/simple_mnist-model-2000.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./explanations/models/'))\n",
    "\n",
    "    saver = tf.train.import_meta_graph('./explanations/models/simple_mnist-model-2000.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./explanations/models/'))\n",
    "    graph = tf.get_default_graph()\n",
    "    X = graph.get_tensor_by_name(\"input:0\")\n",
    "    Y = graph.get_tensor_by_name(\"output:0\")\n",
    "    target_tensor = model(X)\n",
    "    y_class = tf.argmax(target_tensor, 1)\n",
    "\n",
    "    xs = input_x_i\n",
    "    ys = input_y_i\n",
    "    print(\"X shape: {}\".format(xs.shape))\n",
    "    print(\"Y shape: {}\".format(ys.shape))\n",
    "    \n",
    "    # Predictions\n",
    "    eval_dict = {X: xs, Y: ys}\n",
    "    predicted_class = sess.run(y_class, feed_dict=eval_dict)\n",
    "    print(\"Predicted Class: {}\".format(predicted_class))\n",
    "    #relevance_scores = di.explain('elrp', target_tensor * ys, X, xs, use_case='image')\n",
    "    relevance_scores = {\n",
    "       'elrp': di.explain('elrp', target_tensor * ys, X, xs, use_case='image'),\n",
    "        'integrated gradient': di.explain('ig', target_tensor * ys, X, xs, use_case='image'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "input_x = [input_x_i.reshape(28, 28)]\n",
    "input_y = input_y_i\n",
    "\n",
    "n_cols = int(len(relevance_scores)) + 1 # +1 to add a column for the original image\n",
    "n_rows = len(input_x) \n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(6*n_cols, 6*n_rows))\n",
    "\n",
    "# set the properties for text\n",
    "font = {'family': 'avenir',\n",
    "        'color':  'white',\n",
    "        'weight': 'normal',\n",
    "        'size': 14,\n",
    "        }\n",
    "\n",
    "fig.patch.set_facecolor('black')\n",
    "for index, xi in enumerate(input_x):\n",
    "    ax = axes.flatten()[index*n_cols]\n",
    "    visualize(xi, cmap='gray', axis=axes[index], \n",
    "              alpha_edges=1.0, alpha_bgcolor=1).set_title('Original Image: {}'.format(input_y[index]), fontdict=font)\n",
    "    for j, r_type in enumerate(relevance_scores):\n",
    "        axj = axes.flatten()[index*n_cols+j+1]\n",
    "        # Remember to reshape the relevance_score matrix as a 2-D array\n",
    "        # Red: highlights positive relevance\n",
    "        # Blue: highlights negative relevance\n",
    "        visualize(relevance_scores[r_type][index].reshape(28, 28), original_input_img=xi, axis=axj, \n",
    "                  percentile=99,  alpha_edges=1.0, \n",
    "                  alpha_bgcolor=0.75).set_title('Relevance Type: \"{}\"'.format(r_type), fontdict=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
